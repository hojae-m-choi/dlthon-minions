{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e19b7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "import os\n",
    "os.chdir('/aiffel/aiffel/dlthon-minions/dlthon-stuart')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e8b799",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data = pd.read_json('./data/test.json').transpose()\n",
    "PRETRAINED_MODEL = \"gogamza/kobart-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be24d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "def preprocess_sentence(sentence): \n",
    "    # \\n을 공백으로 바꾸기\n",
    "    sentence = re.sub(\"\\n\", \" \", sentence)\n",
    "    \n",
    "    # (ㄱ-ㅎ, ㅏ-ㅣ, \".\", \"?\", \"!\", \",\", ' ')를 제외한 모든 문자를 없애기\n",
    "    sentence = re.sub(\"[^ㄱ-ㅣ가-힣.?!, ]\", \"\", sentence)\n",
    "    \n",
    "    # 단어와 구두점(punctuation) 사이에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "preprocessed = submission_data['text'].apply(preprocess_sentence).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf43079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BartModel, BartConfig\n",
    "\n",
    "# BART 분류기 클래스 정의하기\n",
    "class BartForSequenceClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(BartForSequenceClassification, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.config = BartConfig.from_pretrained(PRETRAINED_MODEL, num_labels=num_labels)\n",
    "        self.bart = BartModel.from_pretrained(PRETRAINED_MODEL, config=self.config)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(self.config.hidden_size, num_labels))\n",
    "        \n",
    "    def forward(self, *args, token_type_ids=None, labels=None, **kwargs, ): #input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.bart(*args, **kwargs,)\n",
    "        pooled_output = outputs[0][:, -1, :]  # 마지막 토큰의 출력 사용\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b1100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 대화 최대 길이 128으로 설정\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "from transformers import BartTokenizerFast, PreTrainedTokenizerFast\n",
    "# 각 conversation을 토큰화하여 새로운 열 'tokenized'에 저장\n",
    "tokenizer=PreTrainedTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "tokenized = tokenizer(\n",
    "    preprocessed, \n",
    "    max_length=MAX_LENGTH,\n",
    "    padding='max_length',  # Pad to the max_length\n",
    "    truncation=True,       # Truncate sequences to the max_length\n",
    "    return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7bffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, StackDataset\n",
    "\n",
    "dataset = StackDataset(**dict(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d861d671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "pred_loader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d61adba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhojae-choi\u001b[0m (\u001b[33maiffel_minions\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/aiffel/aiffel/dlthon-minions/share/wandb/run-20240626_235444-b9zjt6k5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart/runs/b9zjt6k5' target=\"_blank\">rare-totem-4</a></strong> to <a href='https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart' target=\"_blank\">https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart/runs/b9zjt6k5' target=\"_blank\">https://wandb.ai/aiffel_minions/dlthon-minions-share_aiffel_dlthon-minions_dlthon-stuart/runs/b9zjt6k5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-epoch-best:v3, 472.60MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:5.6\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('aiffel_minions/DLthon_finetune_koBart/model-epoch-best:v3', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f11f987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "model_path = os.path.join(artifact_dir, 'model_epochbest.pth')\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "model = BartForSequenceClassification(5)\n",
    "\n",
    "# Apply the state dictionary to the model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "810fbaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [02:19<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "# Now the model is ready to use\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "from tqdm import tqdm\n",
    "predictions =[]\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(pred_loader):\n",
    "        predictions.append(model(**batch)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82a56f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = torch.concat(predictions, axis=0).argmax(axis = 1).numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8100d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'class'를 'type'으로 매핑하는 딕셔너리 생성하기\n",
    "class_to_type = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3,\n",
    "    '일반 대화': 4\n",
    "}\n",
    "type_to_class = {key: idx for idx, key in class_to_type.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16337abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_predictions = [type_to_class[pred] for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "04a9ca2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_df=pd.DataFrame(predictions, columns=['class'])\n",
    "pred_df['file_name'] = submission_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7584d25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>t_000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>t_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>t_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>t_003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>t_004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2</td>\n",
       "      <td>t_495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0</td>\n",
       "      <td>t_496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "      <td>t_497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>2</td>\n",
       "      <td>t_498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0</td>\n",
       "      <td>t_499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class file_name\n",
       "0        3     t_000\n",
       "1        2     t_001\n",
       "2        3     t_002\n",
       "3        1     t_003\n",
       "4        0     t_004\n",
       "..     ...       ...\n",
       "495      2     t_495\n",
       "496      0     t_496\n",
       "497      1     t_497\n",
       "498      2     t_498\n",
       "499      0     t_499\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2948f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('submissions/submission_kobart_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba645f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
